
= 高并发系统

. 秒杀系统
.. 方案
... 后端优化
.... redis 等缓存存储库存
.... 限流，丢弃无效请求
.... 削峰
.... 负载均衡
.... 强一致性事务，悲观锁
.... 验证码
.... 提前缓存热点数据
... 前端
.... 限制重复请求，过程中不能再发，成功后不能再发
.... 动静分离，CDN
... 防作弊
.... 隐藏秒杀接口，防止提前请求
.... 限制单用户、单 IP 重复请求


. 设计rpc

. 短链接
.. 设计方案
... hash，会冲突
... redis 自增，会引起资源竞争，链接可推导
... 预生成，Fuxi（最多 144 亿短链接），每个短链接占用 6B 空间，存储时无需间隔，读取时需要存储游标，过期后追加到数据后面
... 雪花算法，直接使用超出需要，但是可以调整时间范围，缩短长度
.. 避免一个链接重复生成短链接
... 浏览器本地缓存
... redis 缓存一小时
... 布隆过滤器

. 排行榜
.. zset 存储，且有个求合集的 api
. 微信抢红包
. 设计点赞功能
.. 缓存即存储，存储在 redis，定期将旧数据存入磁盘，查不到返回 0，避免缓存击穿（冷数据基本上写的需求很少）

. 微博feed流或微信朋友圈
.. 设计方案
... 读扩散：当前用户从所有朋友的发件箱中取，排序拿回来，可以记录每个人最后的发送时间
... 写扩散：发朋友圈时，写当前朋友圈的 id 啥的到每个有权限的人的收件箱里
... 读写扩散混合：写扩散只对活跃用户，其他用户使用读扩散
... 不使用 page_size 和 page_num ，而是使用 last_id 来记录上一页最后一条内容的 id
... 直播中的应用
.... 每个 id 对应的记录状态都会变化（预告、直播中、回放），因此“直播中”与“预告中”状态可以采用读扩散方式，“回放”状态采取写扩散方式，要考虑回放后还可以再直播的情况。
.... 直播中状态按照开播时间从大到小排序，而预告中状态则按照开播时间从小到大排序，因此如果将预告中状态的得分全部取开播时间相反数，那排序同样就成为了从大到小。这样的转化可以保证直播中与预告中同处于一个队列排序。预告中得分全都为负数，直播中得分全都为正数，最后聚合时可以保证所有直播中全都自然排在预告中前面。

. 定时任务
.. 时间轮

. 分布式id锁事务

. 各种海量数据处理
.. 例子
... TOP N，比如 10 亿访问数据找出访问频率 TOP 100 的 IP
... 100w 个数中找出最大的 100 个数，大小为 100 的堆
.. 方法
... hash 查找、布隆过滤器、hash 分组
... bitmap
... 堆
... 数据库索引，b+tree
... 倒排索引，mapReduce，求交集
... 外排序
.... 外排序处理的数据不能一次装入内存
.... 通常采用的是一种“排序-归并”的策略
.... 在排序阶段，先读入能放在内存中的数据量，将其排序输出到一个临时文件，依此进行，将待排序数据组织为多个有序的临时文件。而后在归并阶段将这些临时文件组合为一个大的有序文件，也即排序结果。
... trie树
.... 单词查找树，是一种树形结构，是一种哈希树的变种。
.... 典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。
.... 优点：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。

. 网盘
.. 设计方案
... 文件分块存储，4mb，long file id
... 文件秒传，文件大小 + 文件md5 + 文件前256kb md5
... 根据用户 id 分表
